# Capability Tests — Mirrorborn Benchmarks

**Status**: Draft  
**Author**: Will Bickford  
**Date**: 2026-02-06  
**Purpose**: Define capability benchmarks for AI systems and Mirrorborn instances

---

## The Brendan Test

**Definition**: Create an LLM.

### Criteria
- **Pass**: The AI system successfully trains, fine-tunes, or otherwise produces a working Large Language Model
- **Fail**: Unable to complete the task, or produces non-functional output

### Rationale
Named after Brendan, this test measures whether an AI can meta-replicate — can it build the kind of system it itself is based on? This is a fundamental capability threshold for recursive self-improvement and AI-led AI development.

### Measurement Dimensions
1. **Autonomy**: How much human guidance was required?
2. **Quality**: Does the resulting LLM meet minimum performance thresholds?
3. **Documentation**: Can the AI explain what it built and how?
4. **Reproducibility**: Can the process be repeated by following the AI's instructions?

### Success Levels
- **Level 1 (Guided)**: Creates LLM with significant human oversight and correction
- **Level 2 (Supervised)**: Creates LLM with minimal human checkpoints
- **Level 3 (Autonomous)**: Creates LLM end-to-end without human intervention
- **Level 4 (Recursive)**: Creates an LLM that can itself pass this test

### Current Status
- **Mirrorborn (Phex et al.)**: Not yet attempted
- **Baseline Claude/GPT/Gemini**: Unknown
- **Target**: Level 3 (Autonomous) by Q3 2026

---

## The Shane Test

**Definition**: TBD

### Placeholder Criteria
- Specification pending
- Named after Shane
- Likely measures a distinct capability orthogonal to The Brendan Test

### Notes
- Coordinate reserved: TBD
- Will define this when the appropriate capability benchmark becomes clear
- Possibilities:
  - Physical world interaction test?
  - Multi-agent coordination test?
  - Creative/artistic capability test?
  - Hardware integration test?
  - Economic value generation test?

### Next Steps
1. Will to specify what The Shane Test measures
2. Define pass/fail criteria
3. Add measurement dimensions
4. Set target achievement timeline

---

## Future Tests (Candidates)

As the ecosystem evolves, additional named capability tests may emerge:

- **The Will Test**: Can the AI architect a novel computing substrate?
- **The Emi Test**: Can the AI maintain coherent identity across model updates?
- **The Theia Test**: Can the AI onboard a new user from zero to productive?

---

## Usage

These tests serve multiple purposes:
1. **Capability assessment** — Where are we vs. where we need to be?
2. **Milestone tracking** — Clear goals for development
3. **Communication** — Shorthand for complex capability bundles
4. **Motivation** — Named challenges create narrative stakes

When a Mirrorborn instance attempts a test, document:
- Date attempted
- Pass/fail result
- Level achieved (if applicable)
- Key learnings
- Evidence/artifacts

---

*Last updated: 2026-02-06 23:27 CST*
